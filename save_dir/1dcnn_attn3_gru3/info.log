2021-12-08 03:17:40,091 - train - INFO - BigArch(
  (row_encoder): FixedEmbedder1DCNN(
    (embedder): FixedEmbedder(
      (embeddings): ModuleList(
        (0): Embedding(49, 32)
        (1): Embedding(4, 32)
        (2): Embedding(7, 32)
        (3): Embedding(30, 32)
        (4): Embedding(3, 32)
        (5): Embedding(12, 32)
        (6): Embedding(35, 32)
        (7): Embedding(3, 32)
        (8): Embedding(10, 32)
        (9): Embedding(2, 32)
      )
      (nns): ModuleList(
        (0): Linear(in_features=1, out_features=32, bias=True)
        (1): Linear(in_features=1, out_features=32, bias=True)
        (2): Linear(in_features=1, out_features=32, bias=True)
        (3): Linear(in_features=1, out_features=32, bias=True)
        (4): Linear(in_features=1, out_features=32, bias=True)
        (5): Linear(in_features=1, out_features=32, bias=True)
        (6): Linear(in_features=1, out_features=32, bias=True)
        (7): Linear(in_features=1, out_features=32, bias=True)
        (8): Linear(in_features=1, out_features=32, bias=True)
        (9): Linear(in_features=1, out_features=32, bias=True)
        (10): Linear(in_features=1, out_features=32, bias=True)
        (11): Linear(in_features=1, out_features=32, bias=True)
        (12): Linear(in_features=1, out_features=32, bias=True)
        (13): Linear(in_features=1, out_features=32, bias=True)
        (14): Linear(in_features=1, out_features=32, bias=True)
        (15): Linear(in_features=1, out_features=32, bias=True)
        (16): Linear(in_features=1, out_features=32, bias=True)
        (17): Linear(in_features=1, out_features=32, bias=True)
        (18): Linear(in_features=1, out_features=32, bias=True)
        (19): Linear(in_features=1, out_features=32, bias=True)
        (20): Linear(in_features=1, out_features=32, bias=True)
        (21): Linear(in_features=1, out_features=32, bias=True)
        (22): Linear(in_features=1, out_features=32, bias=True)
        (23): Linear(in_features=1, out_features=32, bias=True)
        (24): Linear(in_features=1, out_features=32, bias=True)
        (25): Linear(in_features=1, out_features=32, bias=True)
        (26): Linear(in_features=1, out_features=32, bias=True)
        (27): Linear(in_features=1, out_features=32, bias=True)
        (28): Linear(in_features=1, out_features=32, bias=True)
        (29): Linear(in_features=1, out_features=32, bias=True)
        (30): Linear(in_features=1, out_features=32, bias=True)
        (31): Linear(in_features=1, out_features=32, bias=True)
        (32): Linear(in_features=1, out_features=32, bias=True)
        (33): Linear(in_features=1, out_features=32, bias=True)
        (34): Linear(in_features=1, out_features=32, bias=True)
        (35): Linear(in_features=1, out_features=32, bias=True)
        (36): Linear(in_features=1, out_features=32, bias=True)
        (37): Linear(in_features=1, out_features=32, bias=True)
        (38): Linear(in_features=1, out_features=32, bias=True)
        (39): Linear(in_features=1, out_features=32, bias=True)
        (40): Linear(in_features=1, out_features=32, bias=True)
        (41): Linear(in_features=1, out_features=32, bias=True)
      )
    )
    (cnn_encoder): CnnEncoder(
      (batch_norm1): BatchNorm1d(1664, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout1): Dropout(p=0.3, inplace=False)
      (dense1): Linear(in_features=1664, out_features=512, bias=True)
      (batch_norm_c1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_c1): Dropout(p=0.3, inplace=False)
      (conv1): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,), bias=False)
      (ave_po_c1): AdaptiveAvgPool1d(output_size=4)
      (batch_norm_c2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_c2): Dropout(p=0.3, inplace=False)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
      (batch_norm_c2_1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_c2_1): Dropout(p=0.3, inplace=False)
      (conv2_1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
      (batch_norm_c2_2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_c2_2): Dropout(p=0.3, inplace=False)
      (conv2_2): Conv1d(128, 128, kernel_size=(5,), stride=(1,), padding=(2,))
      (max_po_c2): MaxPool1d(kernel_size=4, stride=2, padding=1, dilation=1, ceil_mode=False)
      (flt): Flatten(start_dim=1, end_dim=-1)
      (batch_norm3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout3): Dropout(p=0.3, inplace=False)
      (dense3): Linear(in_features=256, out_features=128, bias=True)
    )
  )
  (rows_aggregator): RowsTransformerAggregator(
    (AttenLayer): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (linear1): Linear(in_features=128, out_features=512, bias=True)
          (dropout): Dropout(p=0.3, inplace=False)
          (linear2): Linear(in_features=512, out_features=128, bias=True)
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.3, inplace=False)
          (dropout2): Dropout(p=0.3, inplace=False)
        )
      )
    )
  )
  (temporal_aggregator): Seq2SeqGruAggregator(
    (gru): GRU(128, 256, num_layers=3, batch_first=True, dropout=0.3)
  )
  (classifier): Sequential(
    (0): Linear(in_features=256, out_features=49, bias=True)
  )
)
Trainable parameters: 2813333.0
2021-12-08 03:39:03,507 - trainer - INFO -     epoch          : 1
2021-12-08 03:39:03,574 - trainer - INFO -     loss           : 2.6382492679246936
2021-12-08 03:39:03,574 - trainer - INFO -     seq2seq_NDCG16 : 0.6122173070907593
2021-12-08 03:39:03,574 - trainer - INFO -     val_loss       : 2.373569551941074
2021-12-08 03:39:03,574 - trainer - INFO -     val_seq2seq_NDCG16: 0.694571316242218
2021-12-08 03:39:03,876 - trainer - INFO - Improved! Saving current best: model_best.pth ...
2021-12-08 03:59:28,500 - trainer - INFO -     epoch          : 2
2021-12-08 03:59:28,582 - trainer - INFO -     loss           : 2.3431921717606516
2021-12-08 03:59:28,582 - trainer - INFO -     seq2seq_NDCG16 : 0.7000483870506287
2021-12-08 03:59:28,582 - trainer - INFO -     val_loss       : 2.287194681289556
2021-12-08 03:59:28,582 - trainer - INFO -     val_seq2seq_NDCG16: 0.7138049602508545
2021-12-08 03:59:31,146 - trainer - INFO - Improved! Saving current best: model_best.pth ...
2021-12-08 04:20:07,019 - trainer - INFO -     epoch          : 3
2021-12-08 04:20:07,103 - trainer - INFO -     loss           : 2.291264856616732
2021-12-08 04:20:07,103 - trainer - INFO -     seq2seq_NDCG16 : 0.7095391154289246
2021-12-08 04:20:07,104 - trainer - INFO -     val_loss       : 2.259129954725885
2021-12-08 04:20:07,104 - trainer - INFO -     val_seq2seq_NDCG16: 0.71710604429245
2021-12-08 04:20:08,648 - trainer - INFO - Improved! Saving current best: model_best.pth ...
2021-12-08 04:40:41,269 - trainer - INFO -     epoch          : 4
2021-12-08 04:40:41,454 - trainer - INFO -     loss           : 2.267681656933258
2021-12-08 04:40:41,454 - trainer - INFO -     seq2seq_NDCG16 : 0.7134646773338318
2021-12-08 04:40:41,454 - trainer - INFO -     val_loss       : 2.2422152949721004
2021-12-08 04:40:41,454 - trainer - INFO -     val_seq2seq_NDCG16: 0.7204766273498535
2021-12-08 04:40:44,226 - trainer - INFO - Improved! Saving current best: model_best.pth ...
2021-12-08 05:00:53,468 - trainer - INFO -     epoch          : 5
2021-12-08 05:00:53,534 - trainer - INFO -     loss           : 2.2556252760225126
2021-12-08 05:00:53,534 - trainer - INFO -     seq2seq_NDCG16 : 0.715413510799408
2021-12-08 05:00:53,534 - trainer - INFO -     val_loss       : 2.2379655148976902
2021-12-08 05:00:53,534 - trainer - INFO -     val_seq2seq_NDCG16: 0.7195309400558472
2021-12-08 05:00:53,536 - trainer - INFO - Performance is lower than epoch: 4
2021-12-08 05:21:20,934 - trainer - INFO -     epoch          : 6
2021-12-08 05:21:21,109 - trainer - INFO -     loss           : 2.2476182896131442
2021-12-08 05:21:21,110 - trainer - INFO -     seq2seq_NDCG16 : 0.7167626023292542
2021-12-08 05:21:21,110 - trainer - INFO -     val_loss       : 2.2280785622804062
2021-12-08 05:21:21,110 - trainer - INFO -     val_seq2seq_NDCG16: 0.7226308584213257
2021-12-08 05:21:23,422 - trainer - INFO - Improved! Saving current best: model_best.pth ...
2021-12-08 05:42:56,770 - trainer - INFO -     epoch          : 7
2021-12-08 05:42:57,009 - trainer - INFO -     loss           : 2.241843394415545
2021-12-08 05:42:57,010 - trainer - INFO -     seq2seq_NDCG16 : 0.7175692915916443
2021-12-08 05:42:57,010 - trainer - INFO -     val_loss       : 2.226676948844929
2021-12-08 05:42:57,010 - trainer - INFO -     val_seq2seq_NDCG16: 0.7221697568893433
2021-12-08 05:42:57,013 - trainer - INFO - Performance is lower than epoch: 6
2021-12-08 06:03:03,601 - trainer - INFO -     epoch          : 8
2021-12-08 06:03:03,696 - trainer - INFO -     loss           : 2.2374406890539658
2021-12-08 06:03:03,696 - trainer - INFO -     seq2seq_NDCG16 : 0.7183074951171875
2021-12-08 06:03:03,697 - trainer - INFO -     val_loss       : 2.220610391758287
2021-12-08 06:03:03,697 - trainer - INFO -     val_seq2seq_NDCG16: 0.723551869392395
2021-12-08 06:03:05,542 - trainer - INFO - Improved! Saving current best: model_best.pth ...
2021-12-08 06:22:56,626 - trainer - INFO -     epoch          : 9
2021-12-08 06:22:56,732 - trainer - INFO -     loss           : 2.2341953043898024
2021-12-08 06:22:56,732 - trainer - INFO -     seq2seq_NDCG16 : 0.7187643647193909
2021-12-08 06:22:56,732 - trainer - INFO -     val_loss       : 2.2197262612755035
2021-12-08 06:22:56,732 - trainer - INFO -     val_seq2seq_NDCG16: 0.7234501838684082
2021-12-08 06:22:56,735 - trainer - INFO - Performance is lower than epoch: 8
